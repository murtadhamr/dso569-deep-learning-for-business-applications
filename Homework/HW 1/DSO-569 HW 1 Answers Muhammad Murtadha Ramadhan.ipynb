{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3831db9a-8f50-4dd2-9d70-2020b3e897cf",
   "metadata": {},
   "source": [
    "**1. Explain the difference between artificial intelligence, machine learning, and deep learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cd200-7381-461d-b305-a277421bbec0",
   "metadata": {},
   "source": [
    "1. **Artificial Intelligence (AI):**\n",
    "   - AI can be described as the effort to automate intellectual tasks normally performed by humans. As such, AI is a general field encompasses machine learning and deep learning, but that also includes many more approaches that may not involve any learning\n",
    "\n",
    "2. **Machine Learning (ML):**\n",
    "   - Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed to perform specific tasks. Machine learning discovers rules for executing a data processing task, given examples of what’s expected.\n",
    "\n",
    "3. **Deep Learning (DL):**\n",
    "   - Deep learning is a specific subfield of machine learning: a new take on learning rep- resentations from data that puts an emphasis on learning successive layers of increas- ingly meaningful representations. The “deep” in “deep learning” isn’t a reference to any kind of deeper understanding achieved by the approach; rather, it stands for this idea of successive layers of representations. How many layers contribute to a model of the data is called the depth of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0838f76-3697-4285-8e58-dacaa4080e3b",
   "metadata": {},
   "source": [
    "**2. Explain supervised learning and unsupervised learning. Give 3 application examples for each type of machine learning method. For each application, give examples of its corresponding loss function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748eb455-91bb-4907-8c03-a38b4ee5f323",
   "metadata": {},
   "source": [
    "**Supervised Learning:**\n",
    "\n",
    "Supervised learning is a type of machine learning where the model is trained on a labeled dataset, meaning each input data point is associated with a corresponding target label. The goal is to learn a mapping function from input variables to output variables. During training, the model adjusts its parameters to minimize the discrepancy between its predictions and the actual labels in the training data.\n",
    "\n",
    "Examples of supervised learning applications:\n",
    "\n",
    "1. **Email Spam Classification**: In this application, the task is to classify emails into spam and non-spam categories. Each email is represented as a set of features (such as the presence of certain keywords) and labeled as either spam or non-spam. Loss Function: Cross-entropy loss.\n",
    "\n",
    "2. **Medical Diagnosis**: Supervised learning can be used to diagnose medical conditions based on patient data such as symptoms, test results, and medical history. Each patient's data is associated with a diagnosis label (e.g., presence or absence of a disease). Loss Function: Binary cross-entropy loss or categorical cross-entropy loss depending on the number of classes.\n",
    "\n",
    "3. **Stock Price Prediction**: Predicting stock prices based on historical stock data, market trends, and other relevant factors. The task involves predicting the future price of a stock given its past performance. Loss Function: Mean squared error (MSE) or mean absolute error (MAE) to measure the difference between predicted and actual stock prices.\n",
    "\n",
    "**Unsupervised Learning:**\n",
    "\n",
    "Unsupervised learning involves training a model on an unlabeled dataset, where the algorithm learns patterns and structures from the input data without explicit supervision. The objective is typically to find hidden patterns, groupings, or representations within the data.\n",
    "\n",
    "Examples of unsupervised learning applications:\n",
    "\n",
    "1. **Clustering Customer Segmentation**: Grouping customers based on their purchasing behavior, demographics, or other features without any prior labels. This can help businesses target specific customer segments with tailored marketing strategies.\n",
    "\n",
    "2. **Anomaly Detection**: Identifying unusual patterns or outliers in data that do not conform to expected behavior. This is useful in various domains such as fraud detection in financial transactions or detecting abnormal behavior in industrial machinery.\n",
    "\n",
    "3. **Dimensionality Reduction**: Reducing the number of features in a dataset while preserving its essential information. Techniques like Principal Component Analysis (PCA) or t-distributed Stochastic Neighbor Embedding (t-SNE) are commonly used for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775deb2d-feb0-4fbc-b162-aa5c505bf4ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "**3. Explain underfitting and overfitting, and optimization and generalization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ef4c5-cc7c-4f23-b065-f5f75d127bbe",
   "metadata": {},
   "source": [
    "1. **Underfitting**:\n",
    "\n",
    "   Underfitting occurs when a model is too simple to capture the underlying structure of the data. In other words, the model fails to learn the patterns present in the training data and performs poorly not only on the training data but also on unseen data (test or validation data). Underfitting typically happens when the model is too simplistic or when it hasn't been trained for long enough.\n",
    "\n",
    "   Characteristics of underfitting:\n",
    "   - High training error\n",
    "   - High testing error\n",
    "   - Poor performance on both training and unseen data\n",
    "\n",
    "2. **Overfitting**:\n",
    "\n",
    "   Overfitting occurs when a model learns the training data too well, capturing noise and random fluctuations rather than the underlying patterns. As a result, an overfitted model performs very well on the training data but generalizes poorly to new, unseen data.\n",
    "\n",
    "   Characteristics of overfitting:\n",
    "   - Low training error (model fits training data very well)\n",
    "   - High testing error (poor performance on unseen data)\n",
    "   - Model captures noise and irrelevant details from the training data\n",
    "\n",
    "**Optimization and Generalization**:\n",
    "\n",
    "Optimization and generalization are two key concepts in machine learning that are closely related to the training and performance of models.\n",
    "\n",
    "1. **Optimization**:\n",
    "\n",
    "   Optimization refers to the process of adjusting the parameters of a model to minimize (or maximize) a loss function. During the training phase, the goal of optimization is to find the set of parameters that yield the best performance on the training data. This is typically achieved through iterative optimization algorithms such as gradient descent, where the model parameters are updated in the direction that reduces the loss function.\n",
    "\n",
    "2. **Generalization**:\n",
    "\n",
    "   Generalization refers to how well a trained model performs on new, unseen data. The ultimate goal of machine learning is to build models that generalize well, meaning they can make accurate predictions on data that they haven't seen during training. A model that generalizes well has learned the underlying patterns in the data rather than memorizing specific instances from the training set. Generalization is crucial because the true performance of a model is evaluated on its ability to make predictions on unseen data.\n",
    "\n",
    "In summary, underfitting and overfitting represent two types of poor model performance, whereas optimization and generalization are key concepts in the training and evaluation of machine learning models. Achieving a balance between fitting the training data well (optimization) and generalizing to unseen data (generalization) is essential for building effective and robust machine learning models. Regularization techniques and appropriate model selection can help mitigate the issues of underfitting and overfitting, leading to better generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c0b2d-91d7-4765-b76c-95f74cfda817",
   "metadata": {},
   "source": [
    "**4. We consider the same stock price data used in class. This data set contains the daily price of 470 stocks from Feb 8, 2013 – Feb 7, 2018. We are interested in predicting whether the return of Adobe Inc (stock symbol: ADBE) will be negative, low, or high using the remaining 469 stocks. Let us use the first 4 years (2/8/2013-2/7/2017) for training and the last year (after 2/8/2017) for testing. Here, a stock has a negative return if the return is negative, a low return if its return falls between 0 and 2%, and a high return if its return exceeds 2%. Follow the steps below to achieve this goal:**\n",
    "- a. Download the Python script from Blackboard used in class for analyzing the same data set. Use the code to convert the stock price data to return data. Modify the code to convert the continuous return to a categorical variable. In particular, the response variable takes the value 1 if negative, 2 if low, and 3 if high.\n",
    "- b. Divide the data into training and test sets.\n",
    "- c. Fit a multinomial regression model. The document below should be able to help. Note\n",
    "that the Python code I used in class works for the binary response, so you should not directly apply it here.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "- d. Use your model in step 3 to predict the value of y on both the training and test data. Note that the model you built returns a probability vector of dimension 3 specifying the probabilities of y=1,2,3 at each time point. For example, if on 1/1/2018, your model returns a probability vector (0.9,0.1,0), then you should set yhat=1 because the probability vector achieves its maximum value at location 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454f885-5d7c-4338-b2bd-52b59428dd8a",
   "metadata": {},
   "source": [
    "**1. Loading the dataset and creating response variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ff7c13-c608-4550-aff7-556be039fad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "data_path = \"https://www.dropbox.com/s/jtecbtn6mktwspr/stock.csv?dl=1\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65f0768-9c60-4071-88d6-dc514bd2c525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>XL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XRX</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>45.08</td>\n",
       "      <td>14.75</td>\n",
       "      <td>78.90</td>\n",
       "      <td>67.8542</td>\n",
       "      <td>36.25</td>\n",
       "      <td>46.89</td>\n",
       "      <td>34.41</td>\n",
       "      <td>73.31</td>\n",
       "      <td>39.12</td>\n",
       "      <td>...</td>\n",
       "      <td>28.24</td>\n",
       "      <td>37.51</td>\n",
       "      <td>88.61</td>\n",
       "      <td>42.87</td>\n",
       "      <td>31.84</td>\n",
       "      <td>27.09</td>\n",
       "      <td>65.30</td>\n",
       "      <td>75.85</td>\n",
       "      <td>24.14</td>\n",
       "      <td>33.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-11</td>\n",
       "      <td>44.60</td>\n",
       "      <td>14.46</td>\n",
       "      <td>78.39</td>\n",
       "      <td>68.5614</td>\n",
       "      <td>35.85</td>\n",
       "      <td>46.76</td>\n",
       "      <td>34.26</td>\n",
       "      <td>73.07</td>\n",
       "      <td>38.64</td>\n",
       "      <td>...</td>\n",
       "      <td>28.31</td>\n",
       "      <td>37.46</td>\n",
       "      <td>88.28</td>\n",
       "      <td>42.84</td>\n",
       "      <td>31.96</td>\n",
       "      <td>27.46</td>\n",
       "      <td>64.55</td>\n",
       "      <td>75.65</td>\n",
       "      <td>24.21</td>\n",
       "      <td>33.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-12</td>\n",
       "      <td>44.62</td>\n",
       "      <td>14.27</td>\n",
       "      <td>78.60</td>\n",
       "      <td>66.8428</td>\n",
       "      <td>35.42</td>\n",
       "      <td>46.96</td>\n",
       "      <td>34.30</td>\n",
       "      <td>73.37</td>\n",
       "      <td>38.89</td>\n",
       "      <td>...</td>\n",
       "      <td>28.41</td>\n",
       "      <td>37.58</td>\n",
       "      <td>88.46</td>\n",
       "      <td>42.87</td>\n",
       "      <td>31.84</td>\n",
       "      <td>27.95</td>\n",
       "      <td>64.75</td>\n",
       "      <td>75.44</td>\n",
       "      <td>24.49</td>\n",
       "      <td>33.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>44.75</td>\n",
       "      <td>14.66</td>\n",
       "      <td>78.97</td>\n",
       "      <td>66.7156</td>\n",
       "      <td>35.27</td>\n",
       "      <td>46.64</td>\n",
       "      <td>34.46</td>\n",
       "      <td>73.56</td>\n",
       "      <td>38.81</td>\n",
       "      <td>...</td>\n",
       "      <td>28.42</td>\n",
       "      <td>37.80</td>\n",
       "      <td>88.67</td>\n",
       "      <td>43.08</td>\n",
       "      <td>32.00</td>\n",
       "      <td>28.26</td>\n",
       "      <td>64.41</td>\n",
       "      <td>76.00</td>\n",
       "      <td>24.74</td>\n",
       "      <td>33.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>44.58</td>\n",
       "      <td>13.99</td>\n",
       "      <td>78.84</td>\n",
       "      <td>66.6556</td>\n",
       "      <td>36.57</td>\n",
       "      <td>46.77</td>\n",
       "      <td>34.70</td>\n",
       "      <td>73.13</td>\n",
       "      <td>38.61</td>\n",
       "      <td>...</td>\n",
       "      <td>28.22</td>\n",
       "      <td>38.44</td>\n",
       "      <td>88.52</td>\n",
       "      <td>42.91</td>\n",
       "      <td>32.12</td>\n",
       "      <td>28.47</td>\n",
       "      <td>63.89</td>\n",
       "      <td>76.34</td>\n",
       "      <td>24.63</td>\n",
       "      <td>33.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 471 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Time      A    AAL    AAP     AAPL   ABBV    ABC    ABT    ACN  \\\n",
       "0  2013-02-08  45.08  14.75  78.90  67.8542  36.25  46.89  34.41  73.31   \n",
       "1  2013-02-11  44.60  14.46  78.39  68.5614  35.85  46.76  34.26  73.07   \n",
       "2  2013-02-12  44.62  14.27  78.60  66.8428  35.42  46.96  34.30  73.37   \n",
       "3  2013-02-13  44.75  14.66  78.97  66.7156  35.27  46.64  34.46  73.56   \n",
       "4  2013-02-14  44.58  13.99  78.84  66.6556  36.57  46.77  34.70  73.13   \n",
       "\n",
       "    ADBE  ...     XL   XLNX    XOM   XRAY    XRX    XYL    YUM    ZBH   ZION  \\\n",
       "0  39.12  ...  28.24  37.51  88.61  42.87  31.84  27.09  65.30  75.85  24.14   \n",
       "1  38.64  ...  28.31  37.46  88.28  42.84  31.96  27.46  64.55  75.65  24.21   \n",
       "2  38.89  ...  28.41  37.58  88.46  42.87  31.84  27.95  64.75  75.44  24.49   \n",
       "3  38.81  ...  28.42  37.80  88.67  43.08  32.00  28.26  64.41  76.00  24.74   \n",
       "4  38.61  ...  28.22  38.44  88.52  42.91  32.12  28.47  63.89  76.34  24.63   \n",
       "\n",
       "     ZTS  \n",
       "0  33.05  \n",
       "1  33.26  \n",
       "2  33.74  \n",
       "3  33.55  \n",
       "4  33.27  \n",
       "\n",
       "[5 rows x 471 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0372313b-30a8-4143-98cc-965242562323",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>XL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XRX</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>72.83</td>\n",
       "      <td>53.88</td>\n",
       "      <td>117.29</td>\n",
       "      <td>167.78</td>\n",
       "      <td>116.34</td>\n",
       "      <td>99.29</td>\n",
       "      <td>62.18</td>\n",
       "      <td>160.46</td>\n",
       "      <td>199.38</td>\n",
       "      <td>...</td>\n",
       "      <td>36.79</td>\n",
       "      <td>72.49</td>\n",
       "      <td>89.07</td>\n",
       "      <td>60.73</td>\n",
       "      <td>32.75</td>\n",
       "      <td>74.84</td>\n",
       "      <td>83.98</td>\n",
       "      <td>128.19</td>\n",
       "      <td>54.98</td>\n",
       "      <td>77.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>71.25</td>\n",
       "      <td>52.10</td>\n",
       "      <td>113.93</td>\n",
       "      <td>160.50</td>\n",
       "      <td>115.17</td>\n",
       "      <td>96.02</td>\n",
       "      <td>61.69</td>\n",
       "      <td>156.90</td>\n",
       "      <td>195.64</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>70.64</td>\n",
       "      <td>84.53</td>\n",
       "      <td>60.06</td>\n",
       "      <td>31.63</td>\n",
       "      <td>75.66</td>\n",
       "      <td>82.63</td>\n",
       "      <td>125.79</td>\n",
       "      <td>54.15</td>\n",
       "      <td>76.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>68.22</td>\n",
       "      <td>49.76</td>\n",
       "      <td>109.86</td>\n",
       "      <td>156.49</td>\n",
       "      <td>109.51</td>\n",
       "      <td>91.90</td>\n",
       "      <td>58.73</td>\n",
       "      <td>151.83</td>\n",
       "      <td>190.27</td>\n",
       "      <td>...</td>\n",
       "      <td>37.68</td>\n",
       "      <td>66.97</td>\n",
       "      <td>79.72</td>\n",
       "      <td>58.54</td>\n",
       "      <td>31.38</td>\n",
       "      <td>72.66</td>\n",
       "      <td>79.80</td>\n",
       "      <td>123.18</td>\n",
       "      <td>51.65</td>\n",
       "      <td>73.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>68.45</td>\n",
       "      <td>51.18</td>\n",
       "      <td>112.20</td>\n",
       "      <td>163.03</td>\n",
       "      <td>111.20</td>\n",
       "      <td>91.54</td>\n",
       "      <td>58.86</td>\n",
       "      <td>154.69</td>\n",
       "      <td>194.47</td>\n",
       "      <td>...</td>\n",
       "      <td>37.34</td>\n",
       "      <td>68.99</td>\n",
       "      <td>78.35</td>\n",
       "      <td>58.46</td>\n",
       "      <td>30.85</td>\n",
       "      <td>71.33</td>\n",
       "      <td>80.58</td>\n",
       "      <td>122.30</td>\n",
       "      <td>52.52</td>\n",
       "      <td>73.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>2018-02-07</td>\n",
       "      <td>68.06</td>\n",
       "      <td>51.40</td>\n",
       "      <td>109.93</td>\n",
       "      <td>159.54</td>\n",
       "      <td>113.62</td>\n",
       "      <td>94.22</td>\n",
       "      <td>58.67</td>\n",
       "      <td>155.15</td>\n",
       "      <td>192.34</td>\n",
       "      <td>...</td>\n",
       "      <td>42.00</td>\n",
       "      <td>66.97</td>\n",
       "      <td>76.94</td>\n",
       "      <td>58.30</td>\n",
       "      <td>31.18</td>\n",
       "      <td>71.79</td>\n",
       "      <td>80.13</td>\n",
       "      <td>120.78</td>\n",
       "      <td>54.02</td>\n",
       "      <td>73.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 471 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time      A    AAL     AAP    AAPL    ABBV    ABC    ABT     ACN  \\\n",
       "1254  2018-02-01  72.83  53.88  117.29  167.78  116.34  99.29  62.18  160.46   \n",
       "1255  2018-02-02  71.25  52.10  113.93  160.50  115.17  96.02  61.69  156.90   \n",
       "1256  2018-02-05  68.22  49.76  109.86  156.49  109.51  91.90  58.73  151.83   \n",
       "1257  2018-02-06  68.45  51.18  112.20  163.03  111.20  91.54  58.86  154.69   \n",
       "1258  2018-02-07  68.06  51.40  109.93  159.54  113.62  94.22  58.67  155.15   \n",
       "\n",
       "        ADBE  ...     XL   XLNX    XOM   XRAY    XRX    XYL    YUM     ZBH  \\\n",
       "1254  199.38  ...  36.79  72.49  89.07  60.73  32.75  74.84  83.98  128.19   \n",
       "1255  195.64  ...  38.25  70.64  84.53  60.06  31.63  75.66  82.63  125.79   \n",
       "1256  190.27  ...  37.68  66.97  79.72  58.54  31.38  72.66  79.80  123.18   \n",
       "1257  194.47  ...  37.34  68.99  78.35  58.46  30.85  71.33  80.58  122.30   \n",
       "1258  192.34  ...  42.00  66.97  76.94  58.30  31.18  71.79  80.13  120.78   \n",
       "\n",
       "       ZION    ZTS  \n",
       "1254  54.98  77.82  \n",
       "1255  54.15  76.78  \n",
       "1256  51.65  73.83  \n",
       "1257  52.52  73.27  \n",
       "1258  54.02  73.86  \n",
       "\n",
       "[5 rows x 471 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847fd2c6-3503-40ac-bbe4-8a988be036e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1258, 469)\n"
     ]
    }
   ],
   "source": [
    "Prices_diff = df.iloc[:, 1:].diff()\n",
    "Prices_diff.dropna(inplace=True)  ## Drop the rows which contain missing values\n",
    "X = pd.DataFrame(Prices_diff.to_numpy()/df.iloc[:(df.shape[0]-1), 1:].to_numpy(),\n",
    "                 columns = df.columns[1:])\n",
    "\n",
    "X['return_tier'] = [1 if i < 0 else 2 if i >= 0 and i <= 0.02 else 3 for i in X['ADBE']]\n",
    "\n",
    "# take the return tier variable as y\n",
    "y = X.loc[:,'return_tier']\n",
    "# print(y)\n",
    "\n",
    "X = X.drop(['ADBE', 'return_tier'], axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7e574f-81b6-40dd-ae44-2b182ccc0e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258\n",
      "469\n"
     ]
    }
   ],
   "source": [
    "# the sample size (number of observations or data points)\n",
    "n = X.shape[0]\n",
    "print(n)\n",
    "\n",
    "# the dimensionality (number of predictor or X variables)\n",
    "p = X.shape[1]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f75abe-7924-4127-add0-2b6fd3b922f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>XL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XRX</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>45.08</td>\n",
       "      <td>14.75</td>\n",
       "      <td>78.90</td>\n",
       "      <td>67.8542</td>\n",
       "      <td>36.25</td>\n",
       "      <td>46.89</td>\n",
       "      <td>34.41</td>\n",
       "      <td>73.31</td>\n",
       "      <td>39.12</td>\n",
       "      <td>...</td>\n",
       "      <td>28.24</td>\n",
       "      <td>37.51</td>\n",
       "      <td>88.61</td>\n",
       "      <td>42.87</td>\n",
       "      <td>31.84</td>\n",
       "      <td>27.09</td>\n",
       "      <td>65.30</td>\n",
       "      <td>75.85</td>\n",
       "      <td>24.14</td>\n",
       "      <td>33.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-11</td>\n",
       "      <td>44.60</td>\n",
       "      <td>14.46</td>\n",
       "      <td>78.39</td>\n",
       "      <td>68.5614</td>\n",
       "      <td>35.85</td>\n",
       "      <td>46.76</td>\n",
       "      <td>34.26</td>\n",
       "      <td>73.07</td>\n",
       "      <td>38.64</td>\n",
       "      <td>...</td>\n",
       "      <td>28.31</td>\n",
       "      <td>37.46</td>\n",
       "      <td>88.28</td>\n",
       "      <td>42.84</td>\n",
       "      <td>31.96</td>\n",
       "      <td>27.46</td>\n",
       "      <td>64.55</td>\n",
       "      <td>75.65</td>\n",
       "      <td>24.21</td>\n",
       "      <td>33.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-12</td>\n",
       "      <td>44.62</td>\n",
       "      <td>14.27</td>\n",
       "      <td>78.60</td>\n",
       "      <td>66.8428</td>\n",
       "      <td>35.42</td>\n",
       "      <td>46.96</td>\n",
       "      <td>34.30</td>\n",
       "      <td>73.37</td>\n",
       "      <td>38.89</td>\n",
       "      <td>...</td>\n",
       "      <td>28.41</td>\n",
       "      <td>37.58</td>\n",
       "      <td>88.46</td>\n",
       "      <td>42.87</td>\n",
       "      <td>31.84</td>\n",
       "      <td>27.95</td>\n",
       "      <td>64.75</td>\n",
       "      <td>75.44</td>\n",
       "      <td>24.49</td>\n",
       "      <td>33.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>44.75</td>\n",
       "      <td>14.66</td>\n",
       "      <td>78.97</td>\n",
       "      <td>66.7156</td>\n",
       "      <td>35.27</td>\n",
       "      <td>46.64</td>\n",
       "      <td>34.46</td>\n",
       "      <td>73.56</td>\n",
       "      <td>38.81</td>\n",
       "      <td>...</td>\n",
       "      <td>28.42</td>\n",
       "      <td>37.80</td>\n",
       "      <td>88.67</td>\n",
       "      <td>43.08</td>\n",
       "      <td>32.00</td>\n",
       "      <td>28.26</td>\n",
       "      <td>64.41</td>\n",
       "      <td>76.00</td>\n",
       "      <td>24.74</td>\n",
       "      <td>33.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>44.58</td>\n",
       "      <td>13.99</td>\n",
       "      <td>78.84</td>\n",
       "      <td>66.6556</td>\n",
       "      <td>36.57</td>\n",
       "      <td>46.77</td>\n",
       "      <td>34.70</td>\n",
       "      <td>73.13</td>\n",
       "      <td>38.61</td>\n",
       "      <td>...</td>\n",
       "      <td>28.22</td>\n",
       "      <td>38.44</td>\n",
       "      <td>88.52</td>\n",
       "      <td>42.91</td>\n",
       "      <td>32.12</td>\n",
       "      <td>28.47</td>\n",
       "      <td>63.89</td>\n",
       "      <td>76.34</td>\n",
       "      <td>24.63</td>\n",
       "      <td>33.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 471 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Time      A    AAL    AAP     AAPL   ABBV    ABC    ABT    ACN  \\\n",
       "0  2013-02-08  45.08  14.75  78.90  67.8542  36.25  46.89  34.41  73.31   \n",
       "1  2013-02-11  44.60  14.46  78.39  68.5614  35.85  46.76  34.26  73.07   \n",
       "2  2013-02-12  44.62  14.27  78.60  66.8428  35.42  46.96  34.30  73.37   \n",
       "3  2013-02-13  44.75  14.66  78.97  66.7156  35.27  46.64  34.46  73.56   \n",
       "4  2013-02-14  44.58  13.99  78.84  66.6556  36.57  46.77  34.70  73.13   \n",
       "\n",
       "    ADBE  ...     XL   XLNX    XOM   XRAY    XRX    XYL    YUM    ZBH   ZION  \\\n",
       "0  39.12  ...  28.24  37.51  88.61  42.87  31.84  27.09  65.30  75.85  24.14   \n",
       "1  38.64  ...  28.31  37.46  88.28  42.84  31.96  27.46  64.55  75.65  24.21   \n",
       "2  38.89  ...  28.41  37.58  88.46  42.87  31.84  27.95  64.75  75.44  24.49   \n",
       "3  38.81  ...  28.42  37.80  88.67  43.08  32.00  28.26  64.41  76.00  24.74   \n",
       "4  38.61  ...  28.22  38.44  88.52  42.91  32.12  28.47  63.89  76.34  24.63   \n",
       "\n",
       "     ZTS  \n",
       "0  33.05  \n",
       "1  33.26  \n",
       "2  33.74  \n",
       "3  33.55  \n",
       "4  33.27  \n",
       "\n",
       "[5 rows x 471 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data training period: 2/8/2013 - 2/7/2017\n",
    "\n",
    "df[(df.Time >='2013-02-08') & (df.Time <= '2017-02-07')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5895a-ec11-4535-98d6-4d84fc293e44",
   "metadata": {},
   "source": [
    "**2. Divide the data into training and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e030ea-8108-430d-8f95-903a2e7fc4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# divide the data into training and testing\n",
    "n_train = 1006\n",
    "n_test = n - n_train\n",
    "\n",
    "X_train = X.iloc[:n_train, :]\n",
    "y_train = y[:n_train]\n",
    "\n",
    "X_test = X.iloc[n_train:, :]\n",
    "y_test = y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c27aa7-7fa7-4551-a2a6-88e4f16ee604",
   "metadata": {},
   "source": [
    "**3. Fitting multinominial regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcdb839c-9155-4e25-9b67-e15ee1f4e981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_fitting_model = LogisticRegression(random_state=0, multi_class='multinomial').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab4a78-5603-4a93-87bc-dd99a78df5b2",
   "metadata": {},
   "source": [
    "**4. Predicting value on both training and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0c36d9-5e41-4524-b419-08c3ec2c5567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhat_train = logit_fitting_model.predict(X_train)\n",
    "yhat_train_prob = logit_fitting_model.predict_proba(X_train)\n",
    "\n",
    "yhat_test = logit_fitting_model.predict(X_test)\n",
    "yhat_test_prob = logit_fitting_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee27a8ea-45e6-49dc-bf75-df7cd5971762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 2 2 2 1 1 2 1]\n",
      "[[0.55049891 0.40293902 0.04656207]\n",
      " [0.38221577 0.52733701 0.09044722]\n",
      " [0.39278209 0.53774917 0.06946874]\n",
      " [0.44237046 0.49417497 0.06345457]\n",
      " [0.38770461 0.55355773 0.05873765]\n",
      " [0.25692891 0.6277563  0.11531479]\n",
      " [0.90889522 0.08821783 0.00288695]\n",
      " [0.77126336 0.21461166 0.01412499]\n",
      " [0.14251007 0.6767797  0.18071023]\n",
      " [0.94315388 0.05528298 0.00156314]]\n",
      "[2 2 2 2 2 2 1 2 2 1]\n",
      "[[0.37236162 0.55149288 0.07614551]\n",
      " [0.22822421 0.62530094 0.14647485]\n",
      " [0.3289157  0.56240863 0.10867567]\n",
      " [0.34952556 0.54827241 0.10220203]\n",
      " [0.42756569 0.49245105 0.07998326]\n",
      " [0.25261295 0.64610262 0.10128443]\n",
      " [0.55920936 0.40032123 0.04046941]\n",
      " [0.38749245 0.53565552 0.07685203]\n",
      " [0.21356212 0.66712875 0.11930913]\n",
      " [0.53956104 0.42030917 0.04012979]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat_train[:10])\n",
    "print(yhat_train_prob[:10])\n",
    "\n",
    "print(yhat_test[:10])\n",
    "print(yhat_test_prob[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d335d-c5bd-4990-a75a-a5c14bda8ff2",
   "metadata": {},
   "source": [
    "**5. Calculating prediction error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d082323-6289-4021-b181-c213c749b9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error on train set: 0.2813121272365805\n",
      "Prediction error on test set: 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction error on train set: {1 - np.mean(y_train == yhat_train)}')\n",
    "print(f'Prediction error on test set: {1 - np.mean(y_test == yhat_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
